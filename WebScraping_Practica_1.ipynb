{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/Extraction_Storage/blob/main/WebScraping_Practica_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_1-4X1ngy7j"
      },
      "source": [
        "\n",
        "<font color=\"red\">*Para iniciar, cree su propia copia de este notebook. File> Save a Copy in Drive*</font>\n",
        "\n",
        "\n",
        "---\n",
        "<font color=\"green\">\n",
        "Ten en cuenta los siguientes pasos:\n",
        "1.    \n",
        "\n",
        "1.   Ve al menú \"Archivo\" en la esquina superior izquierda de la interfaz de Colab.\n",
        "2.   Selecciona la opción \"Guardar una copia en Drive\" del menú desplegable. Esto creará una copia del cuaderno en tu propia cuenta de Google Drive.\n",
        "3. Google Colab abrirá automáticamente la nueva copia del cuaderno, y verás que ahora está ubicado en tu Google Drive en la carpeta \"Colab Notebooks\".\n",
        "4. Puedes editar y ejecutar esta copia del cuaderno como desees, y todos los cambios que hagas se guardarán en tu propia cuenta de Google Drive.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQoB26XYb61"
      },
      "source": [
        "#Web Scraping Práctica 1\n",
        "**Página Books**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNpqHf6IYiIn"
      },
      "source": [
        "A continuación realizaremos la primer práctica de Web Scraping: Extracción de Datos en la Web, sobre un sitio de prueba de Web Scraping."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Preparar  Colab**\n"
      ],
      "metadata": {
        "id": "O8Du-Mw1uwZb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CaOedinHk50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b3d3e5-6515-4137-80b7-5f39531508b1"
      },
      "source": [
        "# Collab trae estas librerías por defecto\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFrx88izRJwX"
      },
      "source": [
        "**2. Importar la librerias**\n",
        "* **Requests**: Esta librería permite realizar peticiones usando cualquiera de los métodos (get, post, put, delete, patch), envío de parámetros y modificaciones de las cabeceras.\n",
        "\n",
        "* **BeautifulSoup o bs4**: es una librería que se utiliza para extraer datos de htmls y xml, esta librería nos facilitará el trabajo a la hora de extraer la información."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFqV6sD1IaJv"
      },
      "source": [
        "# Importar las librerías necesarias\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Página Web objetivo**\n",
        "\n",
        "Para nuestra práctica usaremos a la URL de un sitio web de demostración que está diseñado para practicar web scraping. No es un sitio comercial real, sino que es una plataforma creada para que desarrolladores y personas interesadas en aprender a realizar web scraping puedan hacerlo sin infringir políticas de uso de sitios web comerciales.\n",
        "```\n",
        "https://books.toscrape.com\n",
        "```\n",
        "***Características del sitio:***\n",
        "\n",
        "*Catálogo de libros*: El sitio muestra una colección de libros con sus títulos, precios, categorías, clasificaciones y descripciones, simulando una tienda en línea de venta de libros.\n",
        "\n",
        "*Práctica segura para scraping*: Como está diseñado para este propósito, puedes realizar scraping de sus páginas sin preocuparte por restricciones legales, ya que no tiene las mismas limitaciones que un sitio comercial real.\n",
        "\n",
        "*Estructura HTML sencilla*: El sitio tiene una estructura HTML relativamente simple y es ideal para aprender cómo extraer datos como:\n",
        "* Títulos de libros.\n",
        "* Precios.\n",
        "* Disponibilidad.\n",
        "* Valoraciones (estrellas).\n",
        "\n"
      ],
      "metadata": {
        "id": "tEdmhP3Flvig"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueC2apB-HxVZ"
      },
      "source": [
        "# Nuestra página objetivo\n",
        "url_objetivo = \"https://books.toscrape.com/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**4. Obtener un request de la URL objetivo y comprobación de la respuesta**\n",
        "\n",
        "\n",
        "```\n",
        "site = requests.get(url_objetivo)\n",
        "\n",
        "```\n",
        "`requests.get(url_objetivo)`: Se envía una solicitud HTTP GET a la URL especificada (url_objetivo) para descargar el contenido de la página web.\n",
        "La variable `site` contiene la respuesta del servidor, incluyendo el código de estado (status code) y el contenido de la página en formato HTML.\n",
        "\n",
        "`if site.status_code == 200`: El código verifica si la solicitud fue exitosa. Un status_code 200 significa que la página fue encontrada correctamente (OK) y se puede proceder a trabajar con su contenido.\n",
        "Si el código no fuera 200 (por ejemplo, un 404 o 500), significaría que hubo un error y no tendría sentido continuar con el scraping.\n",
        "\n"
      ],
      "metadata": {
        "id": "9iexCcIzfuhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener un requests de la URL objetivo\n",
        "site = requests.get(url_objetivo)\n",
        "\n",
        "# Si el Status Code es OK!\n",
        "if site.status_code == 200:\n",
        "  # Hacer al Site un soup\n",
        "  soup = BeautifulSoup(site.text)\n"
      ],
      "metadata": {
        "id": "bhng9n08iGdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**5. Convertir el contenido HTML en un objeto BautifulSoup**\n",
        "\n",
        "\n",
        "```\n",
        "soup = BeautifulSoup(site.text)\n",
        "\n",
        "```\n",
        "\n",
        "`BeautifulSoup(site.text)`: Convierte el contenido HTML obtenido (contenido de la página) en un objeto BeautifulSoup.\n",
        "\n",
        "`site.text`: Es el contenido de la página en formato texto (el HTML).\n",
        "`BeautifulSoup` permite analizar el contenido HTML y facilita el acceso a sus etiquetas y atributos."
      ],
      "metadata": {
        "id": "9_d42R7YhKuL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Zg8bOyajA4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Obtener todas las etiquetas de enlace '< a >' en la web:**\n",
        "\n",
        "\n",
        "```\n",
        "tags = soup('a')\n",
        "\n",
        "```\n",
        "\n",
        "`soup('a')`: Busca todas las etiquetas < a > en el documento HTML. Las etiquetas < a > son usadas para crear enlaces (hipervínculos) en HTML.\n",
        "Esta línea guarda una lista de todas las etiquetas < a > en la variable tags."
      ],
      "metadata": {
        "id": "suKeSO7JjAfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Recorrer con un ciclo para imprimir el contenido de los enlaces:**\n",
        "\n",
        "\n",
        "```\n",
        "for tag in tags:\n",
        "    print(tag.get('href'))\n",
        "```\n",
        "\n",
        "\n",
        "`for tag in tags:` Recorre cada etiqueta < a > (enlace) encontrada en el sitio.\n",
        "\n",
        "`tag.get('href')`: Obtiene el valor del atributo href de cada etiqueta < a >. Este atributo contiene la URL a la que apunta el enlace.\n",
        "\n",
        "`print(tag.get('href'))`: Imprime la URL del enlace en la consola. Si un enlace no tiene el atributo href, tag.get('href') devolverá None."
      ],
      "metadata": {
        "id": "7m6ravcikKpy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi5PzkZhY97s"
      },
      "source": [
        "\n",
        "  # Obtener todas las etiquetas de enlace que hay en la web\n",
        "  tags = soup('a')\n",
        "\n",
        "  # Recorrer con un ciclo para imprimir el contenido\n",
        "  for tag in tags:\n",
        "      print(tag.get('href'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Prueba la siguiente línea. Qué imprime?\n",
        "  for tag in tags:\n",
        "      print(tag.get_text())"
      ],
      "metadata": {
        "id": "5Quh3SlbsDhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actividad:**\n",
        "\n",
        "Ahora que ya comprendiste el código, prueba imprimiendo otros elementos de este sitio.\n",
        "\n",
        "Ej. `<title>, <h1>,  <div>`\n",
        "\n",
        "Ingresa al sitio https://books.toscrape.com/ y con Ctrl+U puedes ver el contenido HTML de la página."
      ],
      "metadata": {
        "id": "wFkE0cmYkXVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manejo de Excepciones:**\n",
        "\n",
        "A continuación un ejemplo de manejo de excepciones de la Pag 8 de la lectura Your First Web Scraper.\n"
      ],
      "metadata": {
        "id": "sOqvaP2Xxj86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ej manejo de excepciones Pag 8 de la lectura Your First Web Scraper\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "try :\n",
        "  html = urlopen ( 'https://books.toscrape.com/')\n",
        "except HTTPError as e:\n",
        "  print (e)\n",
        "except URLError as e:\n",
        "  print ( 'The server could not be found!')\n",
        "# return null, break, or do some other \"Plan B\"\n",
        "else :\n",
        "# program continues. Note: If you return or break in the\n",
        "# exception catch, you do not need to use the \"else\" statement\n",
        "  print('Good job!')"
      ],
      "metadata": {
        "id": "Qa4AOfOWxrbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consideraciones**:\n",
        "\n",
        "\n",
        "*   Asegúrate de respetar el archivo robots.txt del sitio web que estás scrapeando, ya que podría tener directivas que limiten o prohíban el scraping.\n",
        "*   Muchos sitios web tienen medidas de protección contra scraping, como bloqueos basados en la frecuencia de solicitudes o CAPTCHAs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k1Oq0A8aldVS"
      }
    }
  ]
}